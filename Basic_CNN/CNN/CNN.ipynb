{\"cells\":[{\"cell_type\":\"code\",\"execution_count\":7,\"id\":\"03a5eee0\",\"metadata\":{},\"outputs\":[],\"source\":[\"#%pip install seaborn\\n\\nimport numpy as np\\nimport os\\nfrom tensorflow.keras.utils import to_categorical\\nfrom sklearn.model_selection import train_test_split\\nfrom PIL import Image\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom sklearn.metrics import confusion_matrix, accuracy_score\\nfrom keras.models import Sequential\\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\\nimport tensorflow as tf\"]},{\"cell_type\":\"code\",\"execution_count\":8,\"id\":\"6caae4c4\",\"metadata\":{},\"outputs\":[{\"name\":\"stdout\",\"output_type\":\"stream\",\"text\":[\"Training data shape: (1680, 256, 256, 3), (1680, 21)\\nValidation data shape: (210, 256, 256, 3), (210, 21)\\nTest data shape: (210, 256, 256, 3), (210, 21)\\n\"]}],\"source\":[\"# ================================\\n# 1. CARGA Y PROCESAMIENTO DEL DATASET\\n# ================================\\n\\ndataset_path = r\\\"D:\\\\Archivos Kmila\\\\Proyecto de grado\\\\dataset\\\\B_1 UC MercedLand\\\\UCMerced_LandUse\\\\Images\\\"\\nnclases = 21\\nimg_size = (256, 256)\\n\\nX = []\\ny = []\\nclase_dict = {}\\n\\nfor idx, clase in enumerate(sorted(os.listdir(dataset_path))):\\n    clase_path = os.path.join(dataset_path, clase)\\n    if os.path.isdir(clase_path):\\n        clase_dict[idx] = clase\\n        for img_name in os.listdir(clase_path):\\n            img_path = os.path.join(clase_path, img_name)\\n            try:\\n                img = Image.open(img_path).convert('RGB')\\n                img = img.resize(img_size)\\n                img_array = np.array(img)\\n                X.append(img_array)\\n                y.append(idx)\\n            except Exception as e:\\n                print(f\\\"Error cargando imagen {img_path}: {e}\\\")\\n\\nX = np.array(X).astype('float32') / 255.0\\ny = np.array(y)\\n\\n# Divisi\\u00f3n 80-10-10\\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\\nX_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\\n\\n# One-hot encoding\\ny_train_cat = to_categorical(y_train, nclases)\\ny_val_cat = to_categorical(y_val, nclases)\\ny_test_cat = to_categorical(y_test, nclases)\\n\\nprint(f\\\"Training data shape: {X_train.shape}, {y_train_cat.shape}\\\")\\nprint(f\\\"Validation data shape: {X_val.shape}, {y_val_cat.shape}\\\")\\nprint(f\\\"Test data shape: {X_test.shape}, {y_test_cat.shape}\\\")\"]},{\"cell_type\":\"code\",\"execution_count\":9,\"id\":\"edb8648c\",\"metadata\":{},\"outputs\":[{\"name\":\"stderr\",\"output_type\":\"stream\",\"text\":[\"c:\\\\Users\\\\kilig\\\\radioconda\\\\Lib\\\\site-packages\\\\keras\\\\src\\\\layers\\\\convolutional\\\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using [...]\\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\\n\",\"c:\\\\Users\\\\kilig\\\\radioconda\\\\Lib\\\\site-packages\\\\keras\\\\src\\\\trainers\\\\data_adapters\\\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)[...]\\n  self._warn_if_super_not_called()\\n\"]},{\"name\":\"stdout\",\"output_type\":\"stream\",\"text\":[\"Epoch 1/100\\n\\u001b[1m53/53\\u001b[0m \\u001b[32m...TRAIN OUTPUT TRUNCATED FOR STORAGE...\\n\"]}],\"source\":[\"# ================================\\n# 2. DEFINICI\\u00d3N DEL MODELO CNN\\n# ================================\\n\\ncnn = Sequential([\\n    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(256, 256, 3)),\\n    BatchNormalization(),\\n    Conv2D(32, (3, 3), activation='relu', padding='same'),\\n    BatchNormalization(),\\n    MaxPooling2D((2, 2)),\\n    Dropout(0.25),\\n\\n    Conv2D(64, (3, 3), activation='relu', padding='same'),\\n    BatchNormalization(),\\n    Conv2D(64, (3, 3), activation='relu', padding='same'),\\n    BatchNormalization(),\\n    MaxPooling2D((2, 2)),\\n    Dropout(0.25),\\n\\n    Conv2D(128, (3, 3), activation='relu', padding='same'),\\n    BatchNormalization(),\\n    Conv2D(128, (3, 3), activation='relu', padding='same'),\\n    BatchNormalization(),\\n    MaxPooling2D((2, 2)),\\n    Dropout(0.25),\\n\\n    Flatten(),\\n    Dense(256, activation='relu'),\\n    BatchNormalization(),\\n    Dropout(0.5),\\n    Dense(128, activation='relu'),\\n    BatchNormalization(),\\n    Dropout(0.5),\\n    Dense(21, activation='softmax')\\n])\\n\\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\\ncnn.compile(optimizer=optimizer,\\n            loss='categorical_crossentropy',\\n            metrics=['accuracy'])\\n\\n# ================================\\n# 3. ENTRENAMIENTO DEL MODELO\\n# ================================\\n\\ndatagen = ImageDataGenerator(\\n    rotation_range=20,\\n    width_shift_range=0.2,\\n    height_shift_range=0.2,\\n    horizontal_flip=True,\\n    zoom_range=0.2,\\n    shear_range=0.2,\\n    fill_mode='nearest'\\n)\\n\\nearly_stopping = tf.keras.callbacks.EarlyStopping(\\n    monitor='val_loss',\\n    patience=10,\\n    restore_best_weights=True\\n)\\n\\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\\n    monitor='val_loss',\\n    factor=0.2,\\n    patience=5,\\n    min_lr=0.00001\\n)\\n\\nhistory = cnn.fit(\\n    datagen.flow(X_train, y_train_cat, batch_size=32),\\n    validation_data=(X_val, y_val_cat),\\n    epochs=100,\\n    callbacks=[early_stopping, reduce_lr]\\n)\\n\"]},{\"cell_type\":\"code\",\"execution_count\":10,\"id\":\"7c8ab029\",\"metadata\":{},\"outputs\":[{\"name\":\"stdout\",\"output_type\":\"stream\",\"text\":[\"\\u001b[1m53/53\\u001b[0m \\u001b[32m4...TRAIN OUTPUT TRUNCATED FOR STORAGE...\\n\\nTraining Accuracy: 91.67%\\nValidation Accuracy: 80.95%\\nTest Accuracy: 87.14%\\n\"]}],\"source\":[\"# ================================\\n# 4. EVALUACI\\u00d3N DEL MODELO\\n# ================================\\n\\ntrain_loss, train_accuracy = cnn.evaluate(X_train, y_train_cat)\\nval_loss, val_accuracy = cnn.evaluate(X_val, y_val_cat)\\ntest_loss, test_accuracy = cnn.evaluate(X_test, y_test_cat)\\n\\nprint(f\\\"\\nTraining Accuracy: {train_accuracy*100:.2f}%\\\")\\nprint(f\\\"Validation Accuracy: {val_accuracy*100:.2f}%\\\")\\nprint(f\\\"Test Accuracy: {test_accuracy*100:.2f}%\\\")\\n\"]},{\"cell_type\":\"code\",\"execution_count\":11,\"id\":\"6ff52a65\",\"metadata\":{},\"outputs\":[{\"data\":{\"image/png\":\"<binary image omitted>\",\"text/plain\":[\"<Figure size 1200x400 with 2 Axes>\"]},\"metadata\":{},\"output_type\":\"display_data\"}],\"source\":[\"# ================================\\n# 5. GR\\u00c1FICAS DE ENTRENAMIENTO\\n# ================================\\n\\nplt.figure(figsize=(12, 4))\\nplt.subplot(1, 2, 1)\\nplt.plot(history.history['accuracy'], label='Train Accuracy')\\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\\nplt.title('Model Accuracy over Epochs')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend()\\n\\nplt.subplot(1, 2, 2)\\nplt.plot(history.history['loss'], label='Train Loss')\\nplt.plot(history.history['val_loss'], label='Validation Loss')\\nplt.title('Model Loss over Epochs')\\nplt.xlabel('Epoch')\\nplt.ylabel('Loss')\\nplt.legend()\\nplt.show()\\n\\n\"]},{\"cell_type\":\"code\",\"execution_count\":12,\"id\":\"f71a3930\",\"metadata\":{},\"outputs\":[{\"name\":\"stdout\",\"output_type\":\"stream\",\"text\":[\"\\u001b[1m7/7\\u001b[0m \\u001b[32m4...OUTPUT TRUNCATED...\\n\"]}],\"source\":[\"# ================================\\n# 6. MATRIZ DE CONFUSI\\u00d3N\\n# ================================\\n\\n y_pred = np.argmax(cnn.predict(X_test), axis=-1)\\n y_true = np.argmax(y_test_cat, axis=-1)\\n\\n conf_matrix = confusion_matrix(y_true, y_pred)\\n plt.figure(figsize=(15, 12))\\n sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\\n            xticklabels=list(clase_dict.values()), \\n            yticklabels=list(clase_dict.values()))\\n plt.title('Confusion Matrix - UC Merced Dataset')\\n plt.xlabel('Predicted Label')\\n plt.ylabel('True Label')\\n plt.show()\\n\"]},{\"cell_type\":\"code\",\"execution_count\":13,\"id\":\"663cfeb8\",\"metadata\":{},\"outputs\":[{\"name\":\"stdout\",\"output_type\":\"stream\",\"text\":[\"\\nAccuracy por clase:\\nagricultural: 80.00%\\nbaseballdiamond: 100.00%\\nbuildings: 90.00%\\ndenseresidential: 80.00%\\nharbor: 100.00%\\nmediumresidential: 90.00%\\nmobilehomepark: 60.00%\\nparkinglot: 90.00%\\nrunway: 100.00%\\nsparseresidential: 100.00%\\nstoragetanks: 50.00%\\ntenniscourt: 50.00%\\nuc_airplane: 100.00%\\nuc_beach: 100.00%\\nuc_chaparral: 100.00%\\nuc_forest: 100.00%\\nuc_freeway: 90.00%\\nuc_golfcourse: 80.00%\\nuc_intersection: 90.00%\\nuc_overpass: 80.00%\\nuc_river: 100.00%\\n\"]}],\"source\":[\"# ================================\\n# 7. PRECISI\\u00d3N POR CLASE\\n# ================================\\n\\nprint(\"\\nAccuracy por clase:\")\\nfor i in range(nclases):\\n    mask = (y_true == i)\\n    if np.sum(mask) > 0:\\n        acc = accuracy_score(y_true[mask], y_pred[mask])\\n        print(f\\\"{clase_dict[i]}: {acc*100:.2f}%\\\")\\n\"]}